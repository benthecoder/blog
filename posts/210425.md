---
title: 'classifying local documents'
tags: 'ML, programming'
date: 'Apr 21, 2025'
---

spent almost the entire day classifying documents with local models

- [google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it)
- [ibm-granite/granite-vision-3.2-2b](https://huggingface.co/ibm-granite/granite-vision-3.2-2b)
- [HuggingFaceTB/SmolVLM-256M-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct)
- [h2oai/h2ovl-mississippi-800m](https://huggingface.co/h2oai/h2ovl-mississippi-800m)
  - a [CPU](https://github.com/navaneeth-algorithm/h2ovl-mississippi-800m-cpu) compatible version

what i learned:

ollama

downloading models, flash attention doesn't work on mac

prompting doesn't work as well on smaller models

always go for the simplest approaches first, how would i solve this without LLMs?

images are slow

CPU is slow
