---
title: 'regression done'
tags: 'learning, AI, career'
date: 'Oct 10, 2024'
---

presenting aha was fun. I was told i was comfortable in front of everyone which is a first. seems like i'm getting better at presenting. never felt nervous but my heart was beating faster and faster uncontrollably that i was losing my breath towards the end. still need to work on a confident voice. i sound like a high school kid that's still not sure about himself, like that kid from yiyi. i do not look or sound ~23.

was so tense and stressed from morning till 3:30. one of the most intense 2 hours and 10 minutes (extra time) of my life. i probably got 20-30% of it wrong, or even more, but i'm glad i got everything done. i was so close to leaving two questions blank, but i powered through, and was able to write down a proof that was sensible.

i realized solving these problems require persistence, they're never obvious from the start, and it requires some thought, it's so easy to give up early on and say "i can't do it". i think i've wired to give up quickly without being patient enough with a problem. i think if you just keep chipping away on the problem, you can get somewhere, and with a bit of luck, and sound intuition, and solid math, you just might reach the end.

i'm also realizing that math skills is so crucial, and i shouldn't skimp on math knowledge, even though i enjoy building and prototyping a lot more. i should put my stats degree to good use, double down on hard math and stats, understand the foundational stuff, because it will pay off in the future when i'm solving harder problems that require deep knowledge.

---

researched into [Hume](https://www.hume.ai/), a series B startup that launched a foundational audio-language model – EVI 2 that integrates empathetic AI into any product

how does the models [work](https://dev.hume.ai/docs/resources/science)? it captures patterns from our speech and face:

- Facial Expression
- Speech Prosody (the way you say words)
- Vocal Bursts (sighs, laughs, oohs, ahhs, umms, and shrieks)
- Emotional Language (the words we say - explicit disclosure of emotion and implicit emotional connotation)

these reveal our preferences – whether we find things interesting or boring, funny or dubious, satisfying or frustrating, and and learns from these signals.

they call it RLHE - reinforcement learning from human expression.

these models trained with RLHE can serve as better copywriters, call center agents, tutors, etc, even in text-only interfaces.

their goal: a future in which technology draws on an understanding of human emotional expression to better serve human goals

they released the first concrete ethical guidelines for empathetic AI : [Hume Initiative](https://www.thehumeinitiative.org/)

they also [publish their research](https://github.com/HumeAI/hume-research-publications/tree/main)

they have an [ai engineer + writer](https://job-boards.greenhouse.io/humeai/jobs/4021608008) role that caught my attention

here's the job description

> We are looking for an engineer and writer to help copyedit developer materials (API documentation, tutorials) and write broad-reaching technical content (blog posts, white papers). In this role, you will create content that helps developers understand the role of emotional intelligence in AI, imagine the future of AI interfaces, and integrate our API into wide-ranging applications. You will work closely with our developer relations, product, engineering, and research teams on a diverse range of content.

> The ideal candidate is a philosophically minded software developer with excellent writing and storytelling skills. They have worked with (but not necessarily written) API documentation, especially for AI tools, have informed opinions about the future of AI, and have a strong interest in science.

currently thinking of building an ios app where you talk to it throughout your day, and it summarizes your thoughts + tags your emotions with hume.ai, something like this [tweet](https://x.com/CherrilynnZ/status/1836881535154409629)
