---
title: 'mixture of experts'
tags: 'journal, ML, resources'
date: 'Dec 30, 2024'
---

a few resource dump on mixture of experts for a project.

from what i understand so far, MoE has two main components: experts and gates. experts are specialized sub-networks, feedforward layers that processes specific portions of input data. gates utilize "gating functions" that are learned mechanisms that decide which experts should handle each input tokens. how? it computes a prob distribution over all experts (softmax) and select the top-k experts for each token based on the probs this selective routing allows MoE to scale # params without proportional increase in computational cost during inference ,as only a subset of experts are active (sparse MoE). the downsides? training can be complex due to the need for effective routing and load balancing among experts. without careful design, some experts may be over or underutilized.

understand

- [Mixture of Experts Explained](https://huggingface.co/blog/moe?utm_source=chatgpt.com)
- [Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer](https://www.youtube.com/watch?v=UiX8K-xBUpE)
- [A Visual Guide to Mixture of Experts (MoE)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

papers

- [LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training](https://arxiv.org/abs/2406.16554)
- [Open mixture-of-experts language models](https://arxiv.org/pdf/2409.02060)

code

- [pytorch impl. of sparsely-gated MOE](https://github.com/lucidrains/mixture-of-experts) by lucid rains
  - ([ST-MoE](https://github.com/lucidrains/st-moe-pytorch))
- [makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch)

more

- [awesome MOE](https://github.com/XueFuzhao/awesome-mixture-of-experts)
