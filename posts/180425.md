---
title: 'finetuning deepseek'
tags: 'ML, AI, programming'
date: 'Apr 18, 2025'
---

[strong compute](https://lu.ma/eptxamdp?tk=7lH72C) hosted a hackathon for ARC AGI and for finetuning deepseek

inspired by this [tweet](https://x.com/OfirPress/status/1896546601063096333) with this prompt: "In pure three.js, without downloading any assets or textures, visualize a spaceship launching from the surface of earth and reaching the surface of the moon."

i wanted to finetune deepseek to generate three.js code.

first i had to understand GRPO

- [Group Relative Policy Optimization (GRPO) - Formula and Code - YouTube](https://www.youtube.com/watch?v=Yi1UCrAsf4o)
- [DeepSeek-R1 Dissection: Understanding PPO & GRPO Without Any Prior Reinforcement Learning Knowledge](https://huggingface.co/blog/NormalUhr/grpo)
- [Bite: How Deepseek R1 was trained](https://www.philschmid.de/deepseek-r1)
- [Recent reasoning research: GRPO tweaks, base model RL, and data curation](https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo)

then it came to understanding how i can finetune deepseek

- [ Fine-tune Deepseek-R1 with a Synthetic Reasoning Dataset](https://huggingface.co/blog/sdiazlor/fine-tune-deepseek-with-a-synthetic-reasoning-data)
- [Fine-Tuning Qwen-0.5B and Llama3.2-1B with GRPO & LLM-J to Outperform OpenAIâ€™s O1-Preview in Q&A - Datawizz](https://datawizz.ai/blog/grpo-fine-tuning-qwen-0-5b-llama-1b-vs-openai-o1-preview#reward-functions-the-innovation-of-llm-judging-llm-j)

i needed data curation, which i used [curator](https://github.com/bespokelabsai/curator/) by [bespokelabs](https://www.bespokelabs.ai) to curate synthetic data with claude 3.7 sonnet with thinking.

they're hosting a [Reasoning Datasets Competition](https://huggingface.co/blog/bespokelabs/reasoning-datasets-competition) that i can submit to

the next essential is the reward functions.

- [Reasoning - GRPO & RL | Unsloth Documentation](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl#reward-functions-verifier)
- [will brown on X: "a beautiful reward function https://t.co/kb5xvRQNtR"](https://x.com/willccbb/status/1894725919400472883)

from what i've gathered, and using claude 3.7 sonnet + extended thinking on web to come up with the rewards, and providing the three.js code generated by deepseek and claude as a reference, i ended up with the rewards below:

```py
combined_score = (
    0.12 * syntax_reward +      # Basic correctness
    0.08 * reasoning_reward +   # Quality of explanation
    0.05 * format_reward +      # Proper formatting
    0.15 * length_reward +      # Appropriate length
    0.15 * creativity_reward +  # Creative solutions
    0.15 * animation_reward +   # Animation quality
    0.10 * performance_reward + # Performance optimizations
    0.08 * responsive_reward +  # Responsive design
    0.10 * interaction_reward + # User interaction
    0.02 * rouge_reward         # Similarity to reference if available
)
```

but this seems overengineered. i need to tweak this more.
